{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score \n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(121), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(121))\n",
    "    ax2.set_yticklabels(np.arange(121))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(32),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5,0.5,0.5], \n",
    "                                                            [0.5,0.5,0.5])])\n",
    "\n",
    "test_transform =transforms.Compose([transforms.Resize(30),\n",
    "                                      transforms.CenterCrop(32),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                           [0.5, 0.5, 0.5])])                                                            \n",
    "\n",
    "data_dir = \"C:/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter 03/21.Plankton classification/datasciencebowl\"\n",
    "train_data = datasets.ImageFolder(data_dir+\"/train/train\", transform=train_transform)  \n",
    "test_data = datasets.ImageFolder(data_dir+\"/test\", transform=test_transform) \n",
    "                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948\n",
      "4075\n"
     ]
    }
   ],
   "source": [
    "trainloader= torch.utils.data.DataLoader(train_data, batch_size = 32, shuffle= True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size = 32, shuffle = False)\n",
    "print(len(trainloader))\n",
    "print(len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485])\n",
    "        std = np.array([0.229])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAI/klEQVR4nO3d7W4UxxYF0CaJsQMxIEVIiBfjUfNAUaRIgWC+gmMC5Pe9mrPJVMbOHrPWzyl1T7tntluqM3XqzufPnzegzzf/9QUAuwknlBJOKCWcUEo4odR3afCnn3466qncv/76a+fr33wz/09KYxyfVI34448/xrGzs7O9z3nnzp3xmHfv3o1jz54923mgbyKUEk4oJZxQSjihlHBCKeGEUrGUcgzSVPk0tf3x48fxGKWUr8fKd2fb5u/I6vnG99n7COBGCCeUEk4oJZxQSjihlHBCqaMvpaQp6mns6upqPObbb78dx5RZjs+nT5+Wxla+Vyvll8S3DUoJJ5QSTiglnFBKOKHU0c/WJtPsWZo5m/oObdu23b17919fE4eXfnCextJs7aGv4+TkZO/zeXJCKeGEUsIJpYQTSgknlBJOKHWrSymT09PTcSy16P/uu/l2+VF8p5bNof3wHW4R4YRSwgmlhBNKCSeUEk4odatLKSst8NMx79+/H8fu37+/93txGCuf83WcUw8h+EoIJ5QSTiglnFBKOKGUcEKpW11KmaQp77Ozs3Hs3bt341hqDJZWs/Dvra48+fDhw9I5r6N0s4snJ5QSTiglnFBKOKGUcEIp4YRS5vj/T9rZ+vvvvx/HVlesrOySzP9K9yp9nsmhSylWpcAtIpxQSjihlHBCKeGEUmZr95B+wJ5mBd+8eTOOnZ+f730dZnL/udV79fHjx3Hsprbe8OSEUsIJpYQTSgknlBJOKCWcUEopZQ9pWv7u3bvjWOpVM+2kbXuHw1gtpfz555/j2MnJyd7nW+lz5MkJpYQTSgknlBJOKCWcUEo4oZRSyh7SdHjajuHevXvj2OXl5c7XpxLLl85nxco/l+7jSk+odO+X+g7tfQRwI4QTSgknlBJOKCWcUEo4oZRSyoGkMktqFjVNy799+3Y8Ju2wvbL1w9cqrSS6uLgYxz59+rTz9dWtHyaenFBKOKGUcEIp4YRSwgmlhBNKKaXsYbXBV1phMh33ww8/jMe8fv16HEslgAcPHoxj0/4ft7n8kva+SZ/n1Pwr7Xy+wpMTSgknlBJOKCWcUEo4oZRwQimllANJW5GvlFlS86lUEklb3P/+++/j2KNHj3a+nsoNx15mSdefVve8evVq5+tnZ2dL7zXx5IRSwgmlhBNKCSeUEk4oZbb2BqTZ2mkbh9XtGFZncn/55Zedr5+fn4/HPHz4cBxLs7zHIP2IferhlLZwSJ/ZxJMTSgknlBJOKCWcUEo4oZRwQqnjnu++BmlbhST9sDmdc5piT1supJJI6j2UyiLTVgLPnz8fj5m2Jdi2bfvxxx/HsbRIoEX6PKcS0vSD+G3bttPT072vof8uwVdKOKGUcEIp4YRSwgmlhBNKHX0pZWVH6TRNnsoD6b3SrsapdDBdS+phk1Y/pNUsqcfNVNJ5+vTpeEzaffvFixfjWFo5M63gaepXNF1jWn2U7tXEkxNKCSeUEk4oJZxQSjihlHBCqaMvpaQp9g8fPux8fdqZeNtyKeXly5fj2OPHj8ex1NxpaoSV/q50vulv3rZcgpnKPanRVWrwdXl5OY6l+ziVI9J7pTJWKn8dujyTVgSlFSsTT04oJZxQSjihlHBCKeGEUsIJpY6+lJJMTZVSuSFNeafyQGrIlVaDTFP9q9P8Jycn41gqOUyrWVLZI5VZ0tjK6o3UaCyVMNJ1JCsN29Lqo9RcbeLJCaWEE0oJJ5QSTiglnFDqVs/WTrNnaeYszSSmmdBD7+S8ui1EkmYTp55FaRuBNEOdFhekGdSpv9C0A/i2bdvr16/HsfRj/9SnKf3dKzPp6bsz8eSEUsIJpYQTSgknlBJOKCWcUOpWl1ImaSo8TaE/efJkHLu6uhrHpm0h0li6xvQD9lTSWSnPpPOlvj6p9JHKLBcXFztfT5/L6nWkUlDa1mLq4ZTKcKncM/HkhFLCCaWEE0oJJ5QSTiglnFBquZSyMi3fsjvx6oqP1RJMMk31pxLAmzdvxrFU+khj06qJdEy6H6vXMfVbSqWNVKZIq0HS6qRU/ppWuvz666/jMWls4skJpYQTSgknlBJOKCWcUEo4odS1rEppL5mk3avTWPq7UvOslZJDKjeksk0qwaTywFSeWS2XpBUa6bhpxU0qe6TPLK2ASeWZdK+ma5xW1Gxb3jJi4skJpYQTSgknlBJOKCWcUEo4odRyKaWlXJJM17ha9khlitXyTLqWSdqZO5Uw0g7bU3kmXXu6jrRHSTpuuh9pf5XV1ULpnCt7s6TPMjWHm3hyQinhhFLCCaWEE0oJJ5SyHcMeYyu7E29b7lk0zVymGc209UP6MffKjOehexJ9yU32VEqfdbpXL1++3Pt8acuIiScnlBJOKCWcUEo4oZRwQinhhFJfZSllVZoqT+WS9IPoqR9NKgHcv39/6TrS2NQzJx2T+vMkq9thTFLZY7Wks7KwI11HKn9NPDmhlHBCKeGEUsIJpYQTSgknlFJK+Y9NpZTr6NGU+vpM75f6DqV+RUkqpUw9i9KqlHS+dFxa+ZNMZZF79+6Nx6x8np6cUEo4oZRwQinhhFLCCaWEE0rVlFIOvZoiHbf6XmlstWnYyjWullnScVMJY3XlyaqVvy2t4Ekrgla3eJjKM+kzWyk7eXJCKeGEUsIJpYQTSgknlBJOKHWjpZTVMkXaXXmlzJLOl6T3mlaXfOn9prFUAkjnS6WlZCodpMZU6b1Wr2Oyuqt4+lzSWLr/FxcXO19Pq1wePHgwjo3XsPcRwI0QTiglnFBKOKGUcEIp4YRSN1pKWV25kcbSdPi0T8bqqo7VlSKH3hvkOpp/HfoaD+06Vumsvt/5+fnO13/77bfxGA2+4BYRTiglnFBKOKGUcEKpmh5CyXXMxq1YnZFtuf5k5RpXZ3hX7uPqvV9ZdPAl04/wU7+ln3/+ee/38eSEUsIJpYQTSgknlBJOKCWcUOooSiktVksHx15mmaRSRCorpL5E01g6X+rds3rvT09Px7FJ6nM09R1KPDmhlHBCKeGEUsIJpYQTSgknlLrT3jsGvlaenFBKOKGUcEIp4YRSwgmlhBNK/Q15fbBCkFlHyAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "imshow(images[10,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 32, 32])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv2_drop = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(64*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 121)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = CNN() \n",
    "criterion = nn.NLLLoss()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "\tIteration: 0\t Loss: 0.1197\n",
      "\tIteration: 40\t Loss: 4.3563\n",
      "\tIteration: 80\t Loss: 4.0605\n",
      "\tIteration: 120\t Loss: 4.1122\n",
      "\tIteration: 160\t Loss: 3.8802\n",
      "\tIteration: 200\t Loss: 3.8593\n",
      "\tIteration: 240\t Loss: 3.8687\n",
      "\tIteration: 280\t Loss: 3.8321\n",
      "\tIteration: 320\t Loss: 3.7839\n",
      "\tIteration: 360\t Loss: 3.7473\n",
      "\tIteration: 400\t Loss: 3.6796\n",
      "\tIteration: 440\t Loss: 3.7150\n",
      "\tIteration: 480\t Loss: 3.6368\n",
      "\tIteration: 520\t Loss: 3.5819\n",
      "\tIteration: 560\t Loss: 3.5774\n",
      "\tIteration: 600\t Loss: 3.5366\n",
      "\tIteration: 640\t Loss: 3.5145\n",
      "\tIteration: 680\t Loss: 3.5073\n",
      "\tIteration: 720\t Loss: 3.4610\n",
      "\tIteration: 760\t Loss: 3.4512\n",
      "\tIteration: 800\t Loss: 3.4325\n",
      "\tIteration: 840\t Loss: 3.4269\n",
      "\tIteration: 880\t Loss: 3.3882\n",
      "\tIteration: 920\t Loss: 3.2890\n",
      "Epoch: 2/10\n",
      "\tIteration: 0\t Loss: 0.0859\n",
      "\tIteration: 40\t Loss: 3.2905\n",
      "\tIteration: 80\t Loss: 3.3083\n",
      "\tIteration: 120\t Loss: 3.2808\n",
      "\tIteration: 160\t Loss: 3.1657\n",
      "\tIteration: 200\t Loss: 3.2272\n",
      "\tIteration: 240\t Loss: 3.1565\n",
      "\tIteration: 280\t Loss: 3.2093\n",
      "\tIteration: 320\t Loss: 3.0264\n",
      "\tIteration: 360\t Loss: 3.1328\n",
      "\tIteration: 400\t Loss: 3.0105\n",
      "\tIteration: 440\t Loss: 3.0750\n",
      "\tIteration: 480\t Loss: 3.0249\n",
      "\tIteration: 520\t Loss: 3.0647\n",
      "\tIteration: 560\t Loss: 3.0970\n",
      "\tIteration: 600\t Loss: 3.0633\n",
      "\tIteration: 640\t Loss: 2.9763\n",
      "\tIteration: 680\t Loss: 2.9932\n",
      "\tIteration: 720\t Loss: 3.1352\n",
      "\tIteration: 760\t Loss: 3.1085\n",
      "\tIteration: 800\t Loss: 2.9345\n",
      "\tIteration: 840\t Loss: 2.9967\n",
      "\tIteration: 880\t Loss: 3.0456\n",
      "\tIteration: 920\t Loss: 2.9701\n",
      "Epoch: 3/10\n",
      "\tIteration: 0\t Loss: 0.0853\n",
      "\tIteration: 40\t Loss: 3.0594\n",
      "\tIteration: 80\t Loss: 3.0215\n",
      "\tIteration: 120\t Loss: 2.8427\n",
      "\tIteration: 160\t Loss: 2.8631\n",
      "\tIteration: 200\t Loss: 2.9257\n",
      "\tIteration: 240\t Loss: 2.9084\n",
      "\tIteration: 280\t Loss: 2.7902\n",
      "\tIteration: 320\t Loss: 2.9656\n",
      "\tIteration: 360\t Loss: 2.8911\n",
      "\tIteration: 400\t Loss: 2.8481\n",
      "\tIteration: 440\t Loss: 2.8256\n",
      "\tIteration: 480\t Loss: 2.8301\n",
      "\tIteration: 520\t Loss: 2.9452\n",
      "\tIteration: 560\t Loss: 2.8361\n",
      "\tIteration: 600\t Loss: 2.9199\n",
      "\tIteration: 640\t Loss: 2.8348\n",
      "\tIteration: 680\t Loss: 2.8096\n",
      "\tIteration: 720\t Loss: 2.7702\n",
      "\tIteration: 760\t Loss: 2.9005\n",
      "\tIteration: 800\t Loss: 2.8154\n",
      "\tIteration: 840\t Loss: 2.8127\n",
      "\tIteration: 880\t Loss: 2.8013\n",
      "\tIteration: 920\t Loss: 2.8508\n",
      "Epoch: 4/10\n",
      "\tIteration: 0\t Loss: 0.0566\n",
      "\tIteration: 40\t Loss: 2.8247\n",
      "\tIteration: 80\t Loss: 2.7836\n",
      "\tIteration: 120\t Loss: 2.7064\n",
      "\tIteration: 160\t Loss: 2.6825\n",
      "\tIteration: 200\t Loss: 2.8583\n",
      "\tIteration: 240\t Loss: 2.8029\n",
      "\tIteration: 280\t Loss: 2.7884\n",
      "\tIteration: 320\t Loss: 2.7166\n",
      "\tIteration: 360\t Loss: 2.7436\n",
      "\tIteration: 400\t Loss: 2.7170\n",
      "\tIteration: 440\t Loss: 2.6793\n",
      "\tIteration: 480\t Loss: 2.7064\n",
      "\tIteration: 520\t Loss: 2.7835\n",
      "\tIteration: 560\t Loss: 2.7755\n",
      "\tIteration: 600\t Loss: 2.7669\n",
      "\tIteration: 640\t Loss: 2.7872\n",
      "\tIteration: 680\t Loss: 2.6961\n",
      "\tIteration: 720\t Loss: 2.6545\n",
      "\tIteration: 760\t Loss: 2.7184\n",
      "\tIteration: 800\t Loss: 2.6796\n",
      "\tIteration: 840\t Loss: 2.7655\n",
      "\tIteration: 880\t Loss: 2.7275\n",
      "\tIteration: 920\t Loss: 2.6680\n",
      "Epoch: 5/10\n",
      "\tIteration: 0\t Loss: 0.0653\n",
      "\tIteration: 40\t Loss: 2.6627\n",
      "\tIteration: 80\t Loss: 2.6767\n",
      "\tIteration: 120\t Loss: 2.5944\n",
      "\tIteration: 160\t Loss: 2.6271\n",
      "\tIteration: 200\t Loss: 2.6359\n",
      "\tIteration: 240\t Loss: 2.6610\n",
      "\tIteration: 280\t Loss: 2.6088\n",
      "\tIteration: 320\t Loss: 2.6492\n",
      "\tIteration: 360\t Loss: 2.6433\n",
      "\tIteration: 400\t Loss: 2.6317\n",
      "\tIteration: 440\t Loss: 2.6830\n",
      "\tIteration: 480\t Loss: 2.5920\n",
      "\tIteration: 520\t Loss: 2.5272\n",
      "\tIteration: 560\t Loss: 2.6640\n",
      "\tIteration: 600\t Loss: 2.7215\n",
      "\tIteration: 640\t Loss: 2.6161\n",
      "\tIteration: 680\t Loss: 2.6035\n",
      "\tIteration: 720\t Loss: 2.6615\n",
      "\tIteration: 760\t Loss: 2.5740\n",
      "\tIteration: 800\t Loss: 2.6214\n",
      "\tIteration: 840\t Loss: 2.6048\n",
      "\tIteration: 880\t Loss: 2.6340\n",
      "\tIteration: 920\t Loss: 2.6727\n",
      "Epoch: 6/10\n",
      "\tIteration: 0\t Loss: 0.0754\n",
      "\tIteration: 40\t Loss: 2.6027\n",
      "\tIteration: 80\t Loss: 2.5394\n",
      "\tIteration: 120\t Loss: 2.6368\n",
      "\tIteration: 160\t Loss: 2.6412\n",
      "\tIteration: 200\t Loss: 2.5894\n",
      "\tIteration: 240\t Loss: 2.5651\n",
      "\tIteration: 280\t Loss: 2.5937\n",
      "\tIteration: 320\t Loss: 2.6077\n",
      "\tIteration: 360\t Loss: 2.5557\n",
      "\tIteration: 400\t Loss: 2.5334\n",
      "\tIteration: 440\t Loss: 2.4847\n",
      "\tIteration: 480\t Loss: 2.5564\n",
      "\tIteration: 520\t Loss: 2.5148\n",
      "\tIteration: 560\t Loss: 2.4359\n",
      "\tIteration: 600\t Loss: 2.5471\n",
      "\tIteration: 640\t Loss: 2.5004\n",
      "\tIteration: 680\t Loss: 2.5534\n",
      "\tIteration: 720\t Loss: 2.5564\n",
      "\tIteration: 760\t Loss: 2.5820\n",
      "\tIteration: 800\t Loss: 2.4441\n",
      "\tIteration: 840\t Loss: 2.4651\n",
      "\tIteration: 880\t Loss: 2.5705\n",
      "\tIteration: 920\t Loss: 2.5881\n",
      "Epoch: 7/10\n",
      "\tIteration: 0\t Loss: 0.0774\n",
      "\tIteration: 40\t Loss: 2.5391\n",
      "\tIteration: 80\t Loss: 2.4680\n",
      "\tIteration: 120\t Loss: 2.4849\n",
      "\tIteration: 160\t Loss: 2.4622\n",
      "\tIteration: 200\t Loss: 2.5470\n",
      "\tIteration: 240\t Loss: 2.5178\n",
      "\tIteration: 280\t Loss: 2.5421\n",
      "\tIteration: 320\t Loss: 2.4744\n",
      "\tIteration: 360\t Loss: 2.5894\n",
      "\tIteration: 400\t Loss: 2.6097\n",
      "\tIteration: 440\t Loss: 2.5743\n",
      "\tIteration: 480\t Loss: 2.4555\n",
      "\tIteration: 520\t Loss: 2.5027\n",
      "\tIteration: 560\t Loss: 2.4251\n",
      "\tIteration: 600\t Loss: 2.5306\n",
      "\tIteration: 640\t Loss: 2.4643\n",
      "\tIteration: 680\t Loss: 2.4995\n",
      "\tIteration: 720\t Loss: 2.5060\n",
      "\tIteration: 760\t Loss: 2.5289\n",
      "\tIteration: 800\t Loss: 2.5162\n",
      "\tIteration: 840\t Loss: 2.4877\n",
      "\tIteration: 880\t Loss: 2.5735\n",
      "\tIteration: 920\t Loss: 2.4812\n",
      "Epoch: 8/10\n",
      "\tIteration: 0\t Loss: 0.0581\n",
      "\tIteration: 40\t Loss: 2.4457\n",
      "\tIteration: 80\t Loss: 2.4947\n",
      "\tIteration: 120\t Loss: 2.4622\n",
      "\tIteration: 160\t Loss: 2.3959\n",
      "\tIteration: 200\t Loss: 2.5020\n",
      "\tIteration: 240\t Loss: 2.4881\n",
      "\tIteration: 280\t Loss: 2.5501\n",
      "\tIteration: 320\t Loss: 2.4853\n",
      "\tIteration: 360\t Loss: 2.3981\n",
      "\tIteration: 400\t Loss: 2.2972\n",
      "\tIteration: 440\t Loss: 2.4142\n",
      "\tIteration: 480\t Loss: 2.5413\n",
      "\tIteration: 520\t Loss: 2.3732\n",
      "\tIteration: 560\t Loss: 2.4175\n",
      "\tIteration: 600\t Loss: 2.4341\n",
      "\tIteration: 640\t Loss: 2.4801\n",
      "\tIteration: 680\t Loss: 2.3933\n",
      "\tIteration: 720\t Loss: 2.4163\n",
      "\tIteration: 760\t Loss: 2.4363\n",
      "\tIteration: 800\t Loss: 2.3786\n",
      "\tIteration: 840\t Loss: 2.4080\n",
      "\tIteration: 880\t Loss: 2.3660\n",
      "\tIteration: 920\t Loss: 2.4548\n",
      "Epoch: 9/10\n",
      "\tIteration: 0\t Loss: 0.0656\n",
      "\tIteration: 40\t Loss: 2.4821\n",
      "\tIteration: 80\t Loss: 2.4142\n",
      "\tIteration: 120\t Loss: 2.4812\n",
      "\tIteration: 160\t Loss: 2.3299\n",
      "\tIteration: 200\t Loss: 2.3219\n",
      "\tIteration: 240\t Loss: 2.3101\n",
      "\tIteration: 280\t Loss: 2.4366\n",
      "\tIteration: 320\t Loss: 2.4084\n",
      "\tIteration: 360\t Loss: 2.3955\n",
      "\tIteration: 400\t Loss: 2.4252\n",
      "\tIteration: 440\t Loss: 2.4173\n",
      "\tIteration: 480\t Loss: 2.4187\n",
      "\tIteration: 520\t Loss: 2.3602\n",
      "\tIteration: 560\t Loss: 2.4640\n",
      "\tIteration: 600\t Loss: 2.3545\n",
      "\tIteration: 640\t Loss: 2.3999\n",
      "\tIteration: 680\t Loss: 2.4754\n",
      "\tIteration: 720\t Loss: 2.4007\n",
      "\tIteration: 760\t Loss: 2.4205\n",
      "\tIteration: 800\t Loss: 2.3743\n",
      "\tIteration: 840\t Loss: 2.4607\n",
      "\tIteration: 880\t Loss: 2.4068\n",
      "\tIteration: 920\t Loss: 2.3596\n",
      "Epoch: 10/10\n",
      "\tIteration: 0\t Loss: 0.0517\n",
      "\tIteration: 40\t Loss: 2.3795\n",
      "\tIteration: 80\t Loss: 2.3606\n",
      "\tIteration: 120\t Loss: 2.4686\n",
      "\tIteration: 160\t Loss: 2.3687\n",
      "\tIteration: 200\t Loss: 2.3324\n",
      "\tIteration: 240\t Loss: 2.4467\n",
      "\tIteration: 280\t Loss: 2.2893\n",
      "\tIteration: 320\t Loss: 2.3671\n",
      "\tIteration: 360\t Loss: 2.4544\n",
      "\tIteration: 400\t Loss: 2.3145\n",
      "\tIteration: 440\t Loss: 2.3905\n",
      "\tIteration: 480\t Loss: 2.4433\n",
      "\tIteration: 520\t Loss: 2.4002\n",
      "\tIteration: 560\t Loss: 2.3629\n",
      "\tIteration: 600\t Loss: 2.3417\n",
      "\tIteration: 640\t Loss: 2.3890\n",
      "\tIteration: 680\t Loss: 2.3359\n",
      "\tIteration: 720\t Loss: 2.3816\n",
      "\tIteration: 760\t Loss: 2.3673\n",
      "\tIteration: 800\t Loss: 2.3566\n",
      "\tIteration: 840\t Loss: 2.4182\n",
      "\tIteration: 880\t Loss: 2.2770\n",
      "\tIteration: 920\t Loss: 2.4112\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "print_every = 40\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   \n",
    "        loss = criterion(output, labels) \n",
    "        loss.backward()                 \n",
    "        optimizer.step()                        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63c38f5843e80bc87def38f0837d4442fb222cf28c891bc8eb07c7420507072b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
