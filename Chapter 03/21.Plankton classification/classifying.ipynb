{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(121), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(121))\n",
    "    ax2.set_yticklabels(np.arange(121))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5], \n",
    "                                                            [0.5])])\n",
    "\n",
    "data_dir = \"C:/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter 03/21.Plankton classification/datasciencebowl/train\"\n",
    "train_data = datasets.ImageFolder(data_dir+\"/train\", transform=train_transform)                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader= torch.utils.data.DataLoader(train_data, batch_size = 32, shuffle= True)\n",
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485])\n",
    "        std = np.array([0.229])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "imshow(images[10,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 3, 1, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 3, 1, 1)\n",
    "        self.conv2_drop = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(64*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = CNN() \n",
    "criterion = nn.CrossEntropyLoss()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 784]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\GitHub\\strive_exer_ai_feb22\\Chapter 03\\21.Plankton classification\\classifying.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/21.Plankton%20classification/classifying.ipynb#ch0000022?line=10'>11</a>\u001b[0m images\u001b[39m.\u001b[39mresize_(images\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39m784\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/21.Plankton%20classification/classifying.ipynb#ch0000022?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/21.Plankton%20classification/classifying.ipynb#ch0000022?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(images)   \u001b[39m# 1) Forward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/21.Plankton%20classification/classifying.ipynb#ch0000022?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels) \u001b[39m# 2) Compute loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/21.Plankton%20classification/classifying.ipynb#ch0000022?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()                  \u001b[39m# 3) Backward pass\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\GitHub\\strive_exer_ai_feb22\\Chapter 03\\21.Plankton classification\\classifying.ipynb Cell 8\u001b[0m in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/21.Plankton%20classification/classifying.ipynb#ch0000022?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/21.Plankton%20classification/classifying.ipynb#ch0000022?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/21.Plankton%20classification/classifying.ipynb#ch0000022?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/21.Plankton%20classification/classifying.ipynb#ch0000022?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    441\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    442\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 443\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    444\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 784]"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 40\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63c38f5843e80bc87def38f0837d4442fb222cf28c891bc8eb07c7420507072b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
