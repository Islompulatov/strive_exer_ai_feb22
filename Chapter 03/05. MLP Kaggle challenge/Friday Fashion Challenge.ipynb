{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "purple-profession",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\"> Friday Fashion Challenge</h2>\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\"> Pytorch and Fashion-MNIST</h2>\n",
    "<br>\n",
    "<div>\n",
    "You'll be using the <a href=\"https://github.com/zalandoresearch/fashion-mnist\" style=\"color:#01ff84>\"> Fashion-MNIST dataset</a>\n",
    ", a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world.\n",
    "</div>\n",
    "<img src='https://res.cloudinary.com/practicaldev/image/fetch/s--tGTLvoST--/c_imagga_scale,f_auto,fl_progressive,h_720,q_auto,w_1280/https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/doc/img/fashion-mnist-sprite.png' width=500px>\n",
    "    \n",
    "<div> \n",
    "<br>\n",
    "Download and import the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adopted-array",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz to C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31.2%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:\\Users\\asus/.pytorch/F_MNIST_data/FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-palestine",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <strong style=\"color:#01ff84\">Helper functions</strong>\n",
    "  <p>1. Visualize a torch vector and a sample prediction:</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "revolutionary-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conscious-effort",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fd2a223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d08962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size   = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size   = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90c2e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(720, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "491d61d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fatty-formula",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 1, 28, 28] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\GitHub\\strive_exer_ai_feb22\\Chapter 03\\05. MLP Kaggle challenge\\Friday Fashion Challenge.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/05.%20MLP%20Kaggle%20challenge/Friday%20Fashion%20Challenge.ipynb#ch0000005?line=9'>10</a>\u001b[0m img \u001b[39m=\u001b[39m images[\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/05.%20MLP%20Kaggle%20challenge/Friday%20Fashion%20Challenge.ipynb#ch0000005?line=10'>11</a>\u001b[0m img\u001b[39m.\u001b[39mshape\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/05.%20MLP%20Kaggle%20challenge/Friday%20Fashion%20Challenge.ipynb#ch0000005?line=11'>12</a>\u001b[0m ps \u001b[39m=\u001b[39m model(img) \u001b[39m# ps stands for probabilities: your model should return values between 0 and 1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/05.%20MLP%20Kaggle%20challenge/Friday%20Fashion%20Challenge.ipynb#ch0000005?line=12'>13</a>\u001b[0m \u001b[39m# that sums to 1. A softmax does this job!\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/05.%20MLP%20Kaggle%20challenge/Friday%20Fashion%20Challenge.ipynb#ch0000005?line=13'>14</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/05.%20MLP%20Kaggle%20challenge/Friday%20Fashion%20Challenge.ipynb#ch0000005?line=14'>15</a>\u001b[0m \u001b[39m# Plot the image and probabilities\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/05.%20MLP%20Kaggle%20challenge/Friday%20Fashion%20Challenge.ipynb#ch0000005?line=15'>16</a>\u001b[0m view_classify(img, ps, version\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFashion\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\asus\\Documents\\GitHub\\strive_exer_ai_feb22\\Chapter 03\\05. MLP Kaggle challenge\\Friday Fashion Challenge.ipynb Cell 8'\u001b[0m in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/05.%20MLP%20Kaggle%20challenge/Friday%20Fashion%20Challenge.ipynb#ch0000037?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/05.%20MLP%20Kaggle%20challenge/Friday%20Fashion%20Challenge.ipynb#ch0000037?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(F\u001b[39m.\u001b[39mmax_pool2d(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x), \u001b[39m2\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/05.%20MLP%20Kaggle%20challenge/Friday%20Fashion%20Challenge.ipynb#ch0000037?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(F\u001b[39m.\u001b[39mmax_pool2d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2_drop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)), \u001b[39m2\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asus/Documents/GitHub/strive_exer_ai_feb22/Chapter%2003/05.%20MLP%20Kaggle%20challenge/Friday%20Fashion%20Challenge.ipynb#ch0000037?line=16'>17</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/asus/anaconda3/envs/deep_learning/lib/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 1, 28, 28] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "# READ HERE! This is just a snippet of the code that can help you. The model has not been\n",
    "# defined above, so if you run this cell you should get an error. This is just to show you\n",
    "# what the output of the function looks like\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[1].view(1, -1)\n",
    "img.shape\n",
    "ps = model(img) # ps stands for probabilities: your model should return values between 0 and 1\n",
    "# that sums to 1. A softmax does this job!\n",
    "\n",
    "# Plot the image and probabilities\n",
    "view_classify(img, ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-hunger",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h3 align=\"center\"> TIPS for the validation part</h3>\n",
    "<br>\n",
    "<div>\n",
    "  <strong style=\"color:#01ff84\">Visualize the most likely class</strong>\n",
    "  <p>With the probabilities, we can get the most likely class using the <code>ps.topk</code> method. This returns the $k$ highest values. Since we just want the most likely class, we can use <code>ps.topk(1)</code>. This returns a tuple of the top-$k$ values and the top-$k$ indices. If the highest value is the fifth element, we'll get back 4 as the index.:</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "uniform-alert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5],\n",
      "        [8],\n",
      "        [4],\n",
      "        [8],\n",
      "        [9],\n",
      "        [1],\n",
      "        [2],\n",
      "        [7],\n",
      "        [2],\n",
      "        [7]])\n"
     ]
    }
   ],
   "source": [
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "# Look at the most likely classes for the first 10 examples\n",
    "print(top_class[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-notion",
   "metadata": {},
   "source": [
    "Now we can check if the predicted classes match the labels. This is simple to do by equating `top_class` and `labels`, but we have to be careful of the shapes. Here `top_class` is a 2D tensor with shape `(batch_size, 1)` while `labels` is 1D with shape `(batch_size)`. To get the equality to work out the way we want, `top_class` and `labels` must have the same shape.\n",
    "\n",
    "If we do\n",
    "\n",
    "```python\n",
    "equals = top_class == labels\n",
    "```\n",
    "\n",
    "`equals` will have shape `(batch_size, batch_size)`, try it yourself. What it's doing is comparing the one element in each row of `top_class` with each element in `labels` which returns 64 (or your batch size) True/False boolean values for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "minimal-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "equals = top_class == labels.view(*top_class.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-variety",
   "metadata": {},
   "source": [
    "If you want to see the indexes of the misclassified images in the batch, you can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = [index for index,value in enumerate(equals) if value.item() is False] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-delicious",
   "metadata": {},
   "source": [
    "Then you can use those indices to index `images[...]`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-publication",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <strong style=\"color:#01ff84\">Compute the accuracy</strong>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-chest",
   "metadata": {},
   "source": [
    "Now we need to calculate the percentage of correct predictions. `equals` has binary values, either 0 or 1. This means that if we just sum up all the values and divide by the number of values, we get the percentage of correct predictions. This is the same operation as taking the mean, so we can get the accuracy with a call to `torch.mean`. If only it was that simple. If you try `torch.mean(equals)`, you'll get an error\n",
    "\n",
    "```\n",
    "RuntimeError: mean is not implemented for type torch.ByteTensor\n",
    "```\n",
    "\n",
    "This happens because `equals` has type `torch.ByteTensor` but `torch.mean` isn't implement for tensors with that type. So we'll need to convert `equals` to a float tensor. Note that when we take `torch.mean` it returns a scalar tensor, to get the actual value as a float we'll need to do `accuracy.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "chemical-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.9375%\n"
     ]
    }
   ],
   "source": [
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "print(f'Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-rapid",
   "metadata": {},
   "source": [
    "Remember that when you do operations over the weights, both in the forward pass and in the optimization step, Pytorch keeps track of them and `autograd`, that is a Pytorch module for automatically calculating the gradients of tensors, will compute the gradient of them! This is useful when you have to perform the backward pass as well, but not for the validation!\n",
    "\n",
    "Why?\n",
    "\n",
    "The reason you compute the gradient is that you want to minimize the loss function, and the gradient allows you to find in which \"direction\" you have to adjust the weights to reach the minumum of the loss function. \n",
    "\n",
    "So this is a step you perform in the training loop, because you want to adjust and learn the weights. In the validation, you use the weights you've learned so far to test how the model is performing, so you don't need to either update the weights or to compute the gradients!\n",
    "\n",
    "You can use a \"context manager\" to turn off the gradients while doing the validation step:\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-texture",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <strong style=\"color:#01ff84\">CAVEAT</strong>\n",
    "<ul>\n",
    " <li> Remember to reset the gradients of the optimizer in the training loop! <br>\n",
    "    <code> optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()</code>\n",
    "    </li>\n",
    " <li> Be careful about the nn.CrossEntropyLoss(): it wants the raw logits as inputs, and not the output of the softmax! I recommend you to use the <code>LogSoftmax</code> as last layer of your network, and the <code>nn.NLLLoss()</code> as criterion. However, to have the probabilities (the ps in the code) you must use <code>torch.exp(output)</code> since the <code>LogSoftmax</code> gives the log of the probabilities!\n",
    " </ul>\n",
    "    \n",
    "</div>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-hurricane",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "   <h3 align=\"center\" style=\"color:#01ff84\"> Save and load a model</h3>\n",
    "    \n",
    "The simplest thing to do is simply save the state dict with <code>torch.save</code>. For example, we can save it to a file <code>'checkpoint.pth'</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "superior-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-equity",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    Then we can load the state dict with <code>torch.load</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "proved-veteran",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-modem",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "\n",
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aquatic-newport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-geology",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "\n",
    "This works only if you have recreated the same exact model (with same layers and so on) and stored it in model, so that it can load the needed information!\n",
    "\n",
    "If you want also to save the epoch and the state of the optimizer, for example, read here https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html.\n",
    "\n",
    "   <h3 align=\"center\" style=\"color:#01ff84\"> Good luck! ðŸ¤ž</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-adobe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-chest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63c38f5843e80bc87def38f0837d4442fb222cf28c891bc8eb07c7420507072b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
